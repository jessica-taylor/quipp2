- Introduction
  - Brief introduction to probabilistic programming
    - Church examples
    - Inference algorithms (Metropolis Hastings)
  - Motivation for Quipp
    - Explanation of "unknown functions"
    - Writing machine learning algorithms as probabilistic programs
    - Accessibility to non-experts
    - Comparison to existing probabilistic programming languages
      - In othel languages, use random variables for parameters
      - Random variables slower because they are updated independently

- Algorithm
  - EM algorithm
    - E step is Metropolis Hastings as in probabilistic programming
    - M step is parameter optimization
  - Types
    - Brief overview of exponential families
    - Gaussian
    - Categorical
    - Other exponential families could be added easily
    - Some algebraic data types
      - Tuples
      - Either
  - Conversion to factor graph
    - Different types of nodes (exponential family)
    - Different types of factors
      - Constant factors
      - Unifying values (e.g. conditioning on compound value equaling something)
      - Factors from known random functions (e.g. gaussian)
      - Factors from unknown function calls
    - Example: clustering conversion to factor graph
  - Inference in factor graph
    - Metropolis hastings
    - Proposal distribution inferred from nearby nodes


- Examples
  - For each example: code, explanation, graph of EM iterations vs. accuracy
  - Efficiency comparisons with other systems (e.g. probabilistic JS, Church)
    - Accuracy graph of each system vs. time

  - Clustering
      dim = 2
      nclusters = 3
      PointType = Vector(dim, Double)
      ClusterType = Categorical(nclusters)

      get_point = rand_function(ClusterType, PointType)

      def sample():
        cluster = Uniform(nclusters)
        return (cluster, get_point(cluster))

  - Naive Bayes
      nfeatures = 10
      FeaturesType = Vector(nfeatures, Bool)

      get_class = rand_function(Unit, Bool)

      class_1_features = rand_function(Unit, FeaturesType)
      class_2_features = rand_function(Unit, FeaturesType)

      def sample():
        which_class = get_class()
        if which_class:
          features = class_1_features()
        else:
          features = class_2_features()
        return (which_class, features)

  - Factor analysis
      num_components = 3
      point_dim = 5

      ComponentsType = Vector(num_components, Double)
      PointType = Vector(point_dim, Double)

      get_point = randFunction(ComponentsType, PointType)

      def sample():
        components = [normal(0, 1) for i in range(num_components)]
        return (comenents, get_point(components))

  - Hidden Markov model
      num_states = 5
      num_observations = 3

      StateType = Categorical(num_states)
      ObsType = Categorical(num_observations)

      trans_fun = rand_function(StateType, StateType)
      obs_fun = rand_function(StateType, ObsType)

      def sample():
        states = [uniform_categorical(num_states)]
        for i in range(9):
          states.append(trans_fun(states[-1]))
        return (states, [obs_fun(s) for s in states])

  - Factorial HMM
      num_states = 2
      num_factors = 3
      num_observations = 3

      StateType = Vector(num_factors, Categorical(num_states))
      ObsType = Categorical(num_observations)

      trans_fun = rand_function(StateType, StateType)
      obs_fun = rand_function(StateType, ObsType)

      def sample():
        states = [uniform_categorical(num_states)]
        for i in range(9):
          states.append(trans_fun(states[-1]))
        return (states, [obs_fun(s) for s in states])

  - Neural network
      input_dim = 100
      hidden_dim = 20
      output_dim = 2

      InputType = Categorical(input_dim)
      HiddenType = Categorical(hidden_dim)
      OutputType = Categorical(output_dim)

      input_to_hidden = rand_function(InputType, HiddenType)
      hidden_to_output = rand_function(HiddenType, OutputType)

      def sample(input_layer):
        hidden_layer = input_to_hidden(input_layer)
        output_layer = hidden_to_output(hidden_layer)
        return ((), output_layer)

  - Latent dirichlet allocation
      n_classes = 10
      n_words = 10000
      n_words_per_document = 1000

      ClassType = Categorical(n_classes)
      WordType = Categorical(n_words)

      class_to_word = rand_function(ClassType, WordType)

      def sample():
          which_class = uniform_categorical(n_classes)
          words = [class_to_word(which_class) for i in range(n_words_per_document]
          return (which_class, words)

  - A more complex example (TODO)
    - Multi-stage: start from simpler model and then add more complexity
    - Show accuracy as the model changes
    - Will discuss this with Andreas

- Future directions
  - Unbounded models
  - Recursive algebraic data types
