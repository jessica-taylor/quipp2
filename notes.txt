EXAMPLES:

logistic regression
linear regression

naive Bayes

Gaussian discriminant analysis

clustering (nd)
hidden markov model (n states)

PCFG

factor analysis

independent component analysis

simple neural network


-----------------
recursion


def x = y,
    y = x;

def frec x = y,
         y = x;



------------------
notes on adts


good forms of recursion

catamorphisms:

f _ _ _ _ x = blah blah only call f on subparts of x


anamorphism:

f _ = CoConstr (anything here including f calls)


what can values be?

AdtDistr VarId [[GraphValue]]


what if a case is applied to AdtDistr?
- expand every case, using the GraphValues in the AdtDistr as the values
- at the end, a "select only this one" factor

but what if the returned values themselves are adts?
then hook up factors of case return values to the result

-----------

parameter inference:

What if we stop finding MLE estimates and instead just assume the parameters come from some prior?

This requires changing our exponential family factors.  This is because currently, the factor passes messages to the arguments (and equivalently parameters) that only care about feature values.  So, for Gaussian we have an exponential likelihood function.  But really, we want a Gaussian likelihood function: extreme values of the argument should reduce the probability of the true data.

Luckily, since we're using Newton's method, we're already computing the Hessian of the likelihood with respect to the parameters.  This quadratic approximation should probably be fine most of the time.

So basically: compute a quadratic approximation of the likelihood with respect to the parameters.  I think we can ignore correlations between different parameters and just find the mean/variance of each parameter.  Then send quadratic messages to them.  So, each parameter should be an independent Gaussian.

This has a nice advantage: since there is no separate optimization step, it is possible to use a system like BUGS to do efficient inference.  It handles parameter uncertainty much better than ordinary MLE.  For example, if we observe 2 values from a Gaussian, the result will be a t distribution rather than a Gaussian distribution.

Note: should look up variational message passing factors and whether what we're doing here is an actual legal factor.


okay so...

lp(y | eta, weights, x) = n * ss(y) - g(n) where n = eta + extend(weights * feat(y))

We want to pass lp(y | eta, weights, x) as a function of x, eta, or weights as a message.

One idea: get it as a gaussian and pass it in.

that is: we compute lp(y | eta, weights, x) as a quadratic function using the gradient and Hessian.

But can we generalize this?  Perhaps x comes from a non-Gaussian distribution, so we would like the message to x to be non-Gaussian.

In general, can we get an approximation to f(x) of the form m * phi(x) + b?

m_i = (f . phi_i^-1)'(phi_i(x)) = f'(x) * phi_i^-1'(phi_i(x))
example: phi_i(x) = x^2
m_i = f'(x)/(2*x)
b = f(x) - m * phi(x)


idea: only care about linear factors for arguments, but care about quadratic ones for parameters.

Think about what this means for estimating a Gaussian.

We have 2 parameters: n1 and n2.  We have a Gaussian distribution over each.

So what do we do if we have n samples and want to estimate paramemters?  Each parameter is involved in n factors.


Factor i will be:

n * [x_i, x_i^2] - g(n) where n = [n1, n2]
where g [n1, n2] = -n1^2/(4*n2)- 1/2 * log (-2 * n2)

n1 * x + n2 * x^2 + n1^2/(4*n2) + 1/2 * log(-2 * n2)

gradient = [x + n1/(2*n2), x^2  - n1^2/(4*n2^2) - 1/(4 * n2)]

diag hessian = [1/(2*n2), 1/(4 * n2^2)]




note about accuracy:

for VMP and pure Gibbs, need accurate messages.  If messages are inaccurate, we can use MH and use the approximate Gibbs samples as proposals.  So we accept that factorNatParam is approximate and that we need to use factorLogValue to actually accept or reject.



-----------------------

parameter testing framework:

What's the best way to test out parameter inference ability?
We want some way to run the model forwards starting from some (random?) parameters, then infer parameters, and see how close we got.

But do we also want to keep track of latent variables?
This is a slight problem, because the program forms a factor graph.  Factor graphs cannot always be sampled from efficiently.  Perhaps we want to mark factors as either Bayesian network nodes or non-Bayesian factors (such as conditions).

-------------------------

How to implement recursive data structures?  We want mu types.  But to do this, we need to have pure eithers.  Namely, in addition to EitherGraphValue, we have PureLeftGraphValue and PureRightGraphValue.  We will also have MuGraphValue.


-------------------------

problem: gaussian nat params are not mean/precision, but rather something like
mean*precision, precision
Why?

mean/precision comes from a normal-gamma distribution.
What distribution should the thing come from?

P(x; mu, prec) = exp(-prec*(x - mu)^2/2 + log(prec))
               = exp(-prec*x^2/2 + prec*mu*x - prec*mu^2/2 + log(prec))
suf stats: prec, prec*mu, log(prec), prec*mu^2

P(x; n1, n2) = exp(n2*x^2 + n1*x + n1^2/(4*n2)-1/2*ln(-2*n2))
suf stats: n2, n1, n1^2/n2, ln(n2)

-------------------------

MH newton method!

What's the probability of getting from a to b?
We start with a Gaussian centered at c but then shift it partway towards a.
Specifically, we divide the translation by a Pareto-distributed random variable.

Prob get from a to b = integral for s from 1 to infinity: pareto_prob(x) * prob get to b from gaussian centered at a + (c - a)/s

----

Case expressions:

case (foo, bar) of
  (X a b c, Y d e) -> ...
  (X a b c, d) -> ...
  (Y a b, e) -> ...

--------

How to handle non-conjugate-exponential factors?
Example: c = a > b, a and b are Gaussian

If we are determining c, it's easy
If we are determining a, we want:
old distr of a / old message * factor ~= new message * old distr of a / old message

-----------------

GADTs

data FingerTree a where
  Nil :: FingerTree a
  Singleton :: a -> FingerTree a
  Branch :: [a] -> [a] -> FingerTree (Node a) -> FingerTree a

How to rewrite to not be recursive?

data FingerTree b a where
  Nil :: FingerTree b a
  Singleton :: a -> FingerTree b a
  Branch :: [a] -> [a] -> b (Node a) -> FingerTree b a

data Mu (f :: * -> *) = Mu (f (Mu f))

cata :: (f a -> a) -> Mu f -> a
cata f (Mu x) = f (fmap (cata f) x)

data Mu2 (f :: (* -> *) -> *) (a :: *) = Mu2 (f (Mu2 f) a)

Mu2 :: ((* -> *) -> * -> *) -> * -> *

data Mu2 (f :: (* -> *) -> * -> *) (a :: *) = Mu2 (f (Mu2 f) a)

Mu2 :: f (Mu2 f) a -> Mu2 f

cata2 :: Functor (f g) => (f g a -> g a) -> Mu2 f a -> g a
cata2 red (Mu (x :: f (Mu2 f) a)) = 
  red (fmap2 (cata2 red) x)

fmap2 :: (Mu2 f a -> g a) -> f (Mu2 f) a -> f g a

fmap2 :: (g a -> h a) -> f g a -> f h a
  


-----------------

idea:

get rid of total functional aspects

have a couple types of recursion

let rec f x ....

let brec 

let urec

easy to support GADTs without weird algebras

let brec {
  f x = x
}

unrestrected mu types

data Mu f = Mu (f (Mu f))
f need not be a functor!

-----------------------

Parametricity?

How many values of exists a. f(a) are there?

exists a. Either (f a) (g a) =
Either (exists a. f a) (exists a. g a)

exists a. (f a, g a) =


How many non-a values can we get out of the expression?
We can feed a's to a's in arguments.

e.g. exists a. (a, a -> b) = b
exists a. (Bool -> a, a -> b) = Bool -> b

--------------------------

probability distributions over types?

clustering =
  let nclusters = geometric 2;
      getCluster = randFunction () : (() -> Categorical nclusters);
      getPoint = randFunction () : (Categorical nclusters -> Vector n);
  \u -> getPoint (getCluster ())
  
slightly saner now:

clustering =
  let nclusters = geometric 2;
  let getCluster = randFunction Unit (Categorical nclusters);
  let getPoint = randFunction (Categorical nclusters) (Vector n);
  \u -> getPoint (getCluster ())

type is a type
what about type inference?
- might have to sacrifice this :(
dynamically typed?
