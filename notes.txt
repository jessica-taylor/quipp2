EXAMPLES:

logistic regression
linear regression

naive Bayes

Gaussian discriminant analysis

clustering (nd)
hidden markov model (n states)

PCFG

factor analysis

independent component analysis

simple neural network


-----------------

idea: further separate types and values
no mu values!
catamorphisms depend on type

type CompoundValueDistr = [(VarId, String, [GraphValue])]

data GraphValue = ... | CompoundGraphValue CompoundValueDistr

unify types with compound values?

ifThenElse pvar (CompoundGraphValue alts1) (CompoundGraphValue alts2) =
  -- find the union of alternatives
  -- for each one appearing in only one, we take an and of different vars
  -- for each one appearing in both, we take ands of different vars for both
     ways we could get it, and then take an if-then-else of both

example:
  x = if xp then A foo else B barx
  y = if yp then B bary else C baz
  if p then x else y =
  if (if p then xp else false) then A
  else if (if p then true else yp) then B (if p then barx else bary)
  else C baz

idea: standardize to alphabetize constructors

type Pat a b = a -> Maybe b

MaybePat :: Pat a b -> Pat (Maybe a) b
MaybePat p (Just x) = p x
MaybePat p Nothing = Nothing


-----------------
recursion


def x = y,
    y = x;

def frec x = y,
         y = x;



------------------
notes on adts


good forms of recursion

catamorphisms:

f _ _ _ _ x = blah blah only call f on subparts of x


anamorphism:

f _ = CoConstr (anything here including f calls)


what can values be?

AdtDistr VarId [[GraphValue]]


what if a case is applied to AdtDistr?
- expand every case, using the GraphValues in the AdtDistr as the values
- at the end, a "select only this one" factor

but what if the returned values themselves are adts?
then hook up factors of case return values to the result

-----------

parameter inference:

What if we stop finding MLE estimates and instead just assume the parameters come from some prior?

This requires changing our exponential family factors.  This is because currently, the factor passes messages to the arguments (and equivalently parameters) that only care about feature values.  So, for Gaussian we have an exponential likelihood function.  But really, we want a Gaussian likelihood function: extreme values of the argument should reduce the probability of the true data.

Luckily, since we're using Newton's method, we're already computing the Hessian of the likelihood with respect to the parameters.  This quadratic approximation should probably be fine most of the time.

So basically: compute a quadratic approximation of the likelihood with respect to the parameters.  I think we can ignore correlations between different parameters and just find the mean/variance of each parameter.  Then send quadratic messages to them.  So, each parameter should be an independent Gaussian.

This has a nice advantage: since there is no separate optimization step, it is possible to use a system like BUGS to do efficient inference.  It handles parameter uncertainty much better than ordinary MLE.  For example, if we observe 2 values from a Gaussian, the result will be a t distribution rather than a Gaussian distribution.

Note: should look up variational message passing factors and whether what we're doing here is an actual legal factor.


okay so...

lp(y | eta, weights, x) = n * ss(y) - g(n) where n = eta + extend(weights * feat(y))

We want to pass lp(y | eta, weights, x) as a function of x, eta, or weights as a message.

One idea: get it as a gaussian and pass it in.

that is: we compute lp(y | eta, weights, x) as a quadratic function using the gradient and Hessian.

But can we generalize this?  Perhaps x comes from a non-Gaussian distribution, so we would like the message to x to be non-Gaussian.

In general, can we get an approximation to f(x) of the form m * phi(x) + b?

m_i = (f . phi_i^-1)'(phi_i(x)) = f'(x) * phi_i^-1'(phi_i(x))
example: phi_i(x) = x^2
m_i = f'(x)/(2*x)
b = f(x) - m * phi(x)


idea: only care about linear factors for arguments, but care about quadratic ones for parameters.

Think about what this means for estimating a Gaussian.

We have 2 parameters: n1 and n2.  We have a Gaussian distribution over each.

So what do we do if we have n samples and want to estimate paramemters?  Each parameter is involved in n factors.


Factor i will be:

n * [x_i, x_i^2] - g(n) where n = [n1, n2]
where g [n1, n2] = -n1^2/(4*n2)- 1/2 * log (-2 * n2)

n1 * x + n2 * x^2 + n1^2/(4*n2) + 1/2 * log(-2 * n2)

gradient = [x + n1/(2*n2), x^2  - n1^2/(4*n2^2) - 1/(4 * n2)]

diag hessian = [1/(2*n2), 1/(4 * n2^2)]




note about accuracy:

for VMP and pure Gibbs, need accurate messages.  If messages are inaccurate, we can use MH and use the approximate Gibbs samples as proposals.  So we accept that factorNatParam is approximate and that we need to use factorLogValue to actually accept or reject.



-----------------------

parameter testing framework:

What's the best way to test out parameter inference ability?
We want some way to run the model forwards starting from some (random?) parameters, then infer parameters, and see how close we got.

But do we also want to keep track of latent variables?
This is a slight problem, because the program forms a factor graph.  Factor graphs cannot always be sampled from efficiently.  Perhaps we want to mark factors as either Bayesian network nodes or non-Bayesian factors (such as conditions).

-------------------------

How to implement recursive data structures?  We want mu types.  But to do this, we need to have pure eithers.  Namely, in addition to EitherGraphValue, we have PureLeftGraphValue and PureRightGraphValue.  We will also have MuGraphValue.


-------------------------

problem: gaussian nat params are not mean/precision, but rather something like
mean*precision, precision
Why?

mean/precision comes from a normal-gamma distribution.
What distribution should the thing come from?

P(x; mu, prec) = exp(-prec*(x - mu)^2/2 + log(prec))
               = exp(-prec*x^2/2 + prec*mu*x - prec*mu^2/2 + log(prec))
suf stats: prec, prec*mu, log(prec), prec*mu^2

P(x; n1, n2) = exp(n2*x^2 + n1*x + n1^2/(4*n2)-1/2*ln(-2*n2))
suf stats: n2, n1, n1^2/n2, ln(n2)

-------------------------

MH newton method!

What's the probability of getting from a to b?
We start with a Gaussian centered at c but then shift it partway towards a.
Specifically, we divide the translation by a Pareto-distributed random variable.

Prob get from a to b = integral for s from 1 to infinity: pareto_prob(x) * prob get to b from gaussian centered at a + (c - a)/s

----

Case expressions:

case (foo, bar) of
  (X a b c, Y d e) -> ...
  (X a b c, d) -> ...
  (Y a b, e) -> ...

--------

How to handle non-conjugate-exponential factors?
Example: c = a > b, a and b are Gaussian

If we are determining c, it's easy
If we are determining a, we want:
old distr of a / old message * factor ~= new message * old distr of a / old message

-----------------

GADTs

data FingerTree a where
  Nil :: FingerTree a
  Singleton :: a -> FingerTree a
  Branch :: [a] -> [a] -> FingerTree (Node a) -> FingerTree a

How to rewrite to not be recursive?

data FingerTree b a where
  Nil :: FingerTree b a
  Singleton :: a -> FingerTree b a
  Branch :: [a] -> [a] -> b (Node a) -> FingerTree b a

data Mu (f :: * -> *) = Mu (f (Mu f))

cata :: (f a -> a) -> Mu f -> a
cata f (Mu x) = f (fmap (cata f) x)

data Mu2 (f :: (* -> *) -> *) (a :: *) = Mu2 (f (Mu2 f) a)

Mu2 :: ((* -> *) -> * -> *) -> * -> *

data Mu2 (f :: (* -> *) -> * -> *) (a :: *) = Mu2 (f (Mu2 f) a)

Mu2 :: f (Mu2 f) a -> Mu2 f

cata2 :: Functor (f g) => (f g a -> g a) -> Mu2 f a -> g a
cata2 red (Mu (x :: f (Mu2 f) a)) = 
  red (fmap2 (cata2 red) x)

fmap2 :: (Mu2 f a -> g a) -> f (Mu2 f) a -> f g a

fmap2 :: (g a -> h a) -> f g a -> f h a
  


-----------------

idea:

get rid of total functional aspects

have a couple types of recursion

let rec f x ....

let brec 

let urec

easy to support GADTs without weird algebras

let brec {
  f x = x
}

unrestrected mu types

data Mu f = Mu (f (Mu f))
f need not be a functor!

-----------------------

Parametricity?

How many values of exists a. f(a) are there?

exists a. Either (f a) (g a) =
Either (exists a. f a) (exists a. g a)

exists a. (f a, g a) =


How many non-a values can we get out of the expression?
We can feed a's to a's in arguments.

e.g. exists a. (a, a -> b) = b
exists a. (Bool -> a, a -> b) = Bool -> b

--------------------------

probability distributions over types?

clustering =
  let nclusters = geometric 2;
      getCluster = randFunction () : (() -> Categorical nclusters);
      getPoint = randFunction () : (Categorical nclusters -> Vector n);
  \u -> getPoint (getCluster ())
  
slightly saner now:

clustering =
  let nclusters = geometric 2;
  let getCluster = randFunction Unit (Categorical nclusters);
  let getPoint = randFunction (Categorical nclusters) (Vector n);
  \u -> getPoint (getCluster ())

type is a type
what about type inference?
- might have to sacrifice this :(
dynamically typed?
-----------------------------

suppose x, y ~ N(0, 1)

ax ~ N(0, a a^T)

ax + by ~ N(0, a a^T + b b^T)

problem:

E = [1, 0; 0, 2]

a = [1, 1]
b = [-1/2, 2]

-> [1.25, 0; 0, 5]

E[(Vf)^T M Vf]
= E[f^T V^T M V F]
= E[f^T V^T R^T M R V f]

Rxx^TR^T

VRf = Vf

VR = rotate each row of V (incl. exchange columns with each other)

columns of V = things multiplied by factors
rows of V = ways of getting components out
rows of V = per-factor loadings for each component

want:
min_R ||RA - B||^2
= min_R tr((RA - B)^T(RA - B))
= min_R tr(A^T R^T R A - B^T R A - A^ T R^T B + B^T B)
= min_R tr(A^T A - B^T R A - A^ T R^T B + B^T B)
= ||A||^2 + ||B||^2 - 2 max_R tr(B^T R A)
= ||A||^2 + ||B||^2 - 2 max_R tr(R A B^T)
= ||A||^2 + ||B||^2 - 2 max_R sum_i sum_k R_{i,k} (AB^T)_{k,i}

= ||A||^2 + ||B||^2 - 2 max_R tr(B^T R A)

want R^T to look like AB^T

= ||A||^2 + ||B||^2 - 2 max_R tr(B^T R A)

A = usv^T,
B = USV^T
tr(B^T R A) =
tr(VSU^T R usv^T

tr(VSRsv^T)




min_R ||AR - B||^2
= min_R tr((AR - B)^T(AR - B))
= min_R tr(R^TA^T A R - B^T A R - R^T A^T B + B^T B)
= ||A||^2 + ||B||^2 - 2 max_R tr(R^T A^T B)

A^T B = USV^T

= ||A||^2 + ||B||^2 - 2 max_R tr(R^T USV^T)
= ||A||^2 + ||B||^2 - 2 max_R tr(V^T R^T US)
= ||A||^2 + ||B||^2 - 2 max_R tr(S)
-> R = UV^T




= min_R tr(A^T A - B^T R A - A^ T R^T B + B^T B)
= ||A||^2 + ||B||^2 - 2 max_R tr(B^T R A)
= ||A||^2 + ||B||^2 - 2 max_R tr(R A B^T)
= ||A||^2 + ||B||^2 - 2 max_R tr(R A^T B)

AB^T

-----

linear approx f(x, y) 
could take gradient at x...
but, what if we have

_/\___
_______/\

if point in middle of likelihood, we will just be sampling from prior!

quadric approximation:

f'(x) = a
f''(x) = b
f(y) = by^2/2 + cx
f'(y) = by + c
f'(x) = bx + c = a
c = a - bx

----------

regularization!

for categorical: start with maxent.

how to get laplace's rule of succession automatically?
problem: harder when we have features...
- normalize based on variance of features?

for linreg: what is maxent?

(mean, variance) -> P(x | mean, variance)

-------

HMM symmetry?

given transition matrix and observation matrix:
can freely swap stetes with each other if consistent
no other symmetries?
  - would require it to be _impossible_ to determine states and observation
    distributions given infinite data
therefore...compute best permutation?
problem....distr might be close but this doesn't guarantee matrices are close


-----

parameter optimization with feature and value variances

want to maximize E[log P(y | x; theta)]
(like before but instead of (x,y) being from a categorical distribution, it's a mixture of products of varitional distributions)

E[log P(y | x; theta)]
= E[psi(X)^T N phi(Y) - g(N^Tpsi(X))]

first term does not depend on variance of X and Y (because independent); just use means
for second term, create quadratic approximation of g at the expectation

E[g(eta)] ~= 

f(eta) = eta^T A eta + b^T eta + c
f'(eta) = 2 A eta + b
f''(eta) = 2A

f(mean_eta) = g(mean_eta)
f'(mean_eta) = g'(mean_eta)
<=>
2A eta + b = g'(mean_eta)

f''(mean_eta) = g''(mean_eta) <=> A = g''(mean_eta) / 2

f'(mean_eta) = g'(mean_eta) <=> 2A mean_eta + b = g'(mean_eta)
<=>
g''(mean_eta) mean_eta + b = g'(mean_eta)
<=>
b = g'(mean_eta) - g''(mean_eta) mean_eta

in other words:
f(mean_eta + x) = g(mean_eta) + g'(mean_eta)^T x + 1/2  x^T g''(mean_eta) x



f(x + y) = g(x) + g'(x)^T y + 1/2 y^T g''(x) y
f(y) = g(x) + g'(x)^T (y - x) + 1/2 (y-x)^T g''(x) (y - x)
= g(x) - 


f(x) = x^T A x

X ~ N(0, E)

E[f(X)] = A * E




