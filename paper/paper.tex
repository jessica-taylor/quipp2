\documentclass[proceed]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multicol} \usepackage{fancyheadings} \usepackage{pdfpages}
\setlength{\emergencystretch}{10em}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
 
\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\DeclareMathOperator*{\Exp}{\mathbb{E}}
\DeclareMathOperator*{\Prob}{\mathbf{P}}


\title{Qualitative Probabilistic Programming}
\author {}
\begin{document}

  \maketitle

  \begin{abstract}
    In probabilistic programs, sometimes it is difficult to specify the correct
    parameterized family of distributions.  We explore an extension to
    probabilistic programming languages that allows programmers to mark some
    distributions as unspecified.  Then, we can fill in the distribution with
    some family and infer parameters.
  \end{abstract}

  \section{Introduction}

  Probabilistic programming languages have made it easier to specify Bayesian
  models as programs.  Once a model is written as a probabilistic
  program, it is possible to use generic inference algorithms (such as
  Metropolis Hastings) to perform inference.

  In this paper, we discuss a feature of probabilistic programming languages
  that makes it easier to specify some models.
  Specifically, we allow the programmer to specify that some random functions
  are unknown.  The system will fill in these unknown functions with
  some reasonable default family of distributions based on the types and
  proceed to infer the parameters to this distribution using the
  expectation maximization algorithm.

  This feature has multiple advantages.  First, it is easier to write
  models without knowing about the class of models being used.  This should
  make probabliistic programming more accessible to non-experts.  Secondly,
  parameter inference is more efficient if specialized algorithms are used
  rather than the generic algorithms used to infer other random variables
  (such as Metropolis Hastings).

  We define an example probabilistic programming language with this feature
  (Quipp) and show how it can be used to write machine learning models
  concisely.

  In an ordinary probabilistic programming language (such as Church),
  it would be possible to treat parameters as random variables.  This
  would allow ordinary inference algorithms to infer parameters.  However,
  there are advantages of having unknown functions as a feature
  in the language.
  First, it is easier to
  write programs without knowing the details of different distributions.
  Second, specialized algorithms can be used to make parameter inference
  faster.

  Inference in these models is performed using the expectation-maximization
  algorithm, with alternating steps of inferring latent variables and optimizing
  parameters.


  - Motivation for Quipp
    - Explanation of "unknown functions"
    - Writing machine learning algorithms as probabilistic programs
    - Accessibility to non-experts
    - Comparison to existing probabilistic programming languages
      - In other languages, use random variables for parameters
      - Random variables slower because they are updated independently

  \section{Syntax}

  Quipp is implemented as a Python library, so Quipp programs are written as Python programs that have access
  to special functions.  Here is an example of a Quipp program to perform clustering:


  \begin{verbatim}
  PointType = Vector(2, Double)
  ClusterType = Categorical(3)

  get_cluster = rand_function(ClusterType)
  get_point = rand_function(ClusterType, PointType)

  def sample():
    cluster = get_cluster()
    return (cluster, get_point(cluster))
  \end{verbatim}

  Notice that we declared two types (\texttt{PointType} and \texttt{ClusterType})
  and two random functions (\texttt{get\_cluster} and \texttt{get\_point}).
  Type annotations are necessary for random functions.  The type
  \texttt{Vector(2, Double)} expresses the fact that the points are 2-dimensional,
  and the type \texttt{Categorical(3)} expresses the fact that there are 3
  possible clusters (so a cluster may be either 0, 1, or 2).  We do not know
  the distribution over clusters or over points given a cluster.
  We will fill in the \texttt{get\_cluster} function with
  a learned categorical distribution, and we will fill in the
  \texttt{get\_point} function with a learned unknown linear function with noise.

  \section{Class of Distributions}

    For unknown functions, the class of functions used is a kind
    of generalized linear model.  We assume that the result
    is distributed from some exponential family.  We label
    some subset of the sufficient statistics as \emph{features}.  The natural
    parameters corresponding to non-features are constant, while natural
    parameters corresponding to features are determined as an affine
    function of the features of the arguments.

    Let $\psi_x$ be a matrix such that $\psi \phi(x)$ is a vector of the
    features of $x$.  Then

    $$p_{\mathbf{N}}(y | x) = \exp(\psi(x)^T N \phi(y) - g(N^T \psi(X)))$$

    Given $(x, y)$ samples, parameter estimation to maximize log probability is a convex
    problem:

    \begin{align*}
      \log p_{\mathbf{N}}(y | x) = \psi(x)^T N \phi(y) - g(N^T \psi(X))
    \end{align*}
    since the log probability function is concave when $g$ is convex (as it is
    for any exponential family distribution).

    Therefore, it is possible to use Newton's method to optimize the parameters.

  \section{Inference}

    To perform inference of both latent variables and parameters, we first
    translate the program to a factor graph.  Next, we run the
    expectation maximization algorithm on this graph, iterating stages of
    estimating latent variables using Metropolis Hastings and parameter
    inference using Newton's method.

    In the factor graph, there are 2 types of variables: real numbers
    and categorical values.
    In Quipp, we have an additonal tuple type, which gets translated
    to multiple underlying variables in the factor graph.

    There are a few different types of factors:
    \begin{enumerate}
      \item Factors representing known distributions
      \item Factors representing unknown distribution
      \item Factors asserting that a value is equal to some constant
    \end{enumerate}

    How is inference in the factor graph performed?  We use the Metropolis Hastings
    algorithm, updating one node at a time.  To get a proposal for a given node,
    we approximate the local distribution using message passing (similar to
    in variational message passing).  Specifically, for each factor
    $f(x_1, x_2, ..., x_n)$ and each $1 \leq i \leq n$, we send a message
    to variable $x_i$ that approximates $x_i \mapsto f(x_1, x_2, ..., x_n)$.



  \section{Examples}

  \subsection{Clustering}
\begin{verbatim}
dim = 2
nclusters = 3

PointType = Vector(dim, Double)
ClusterType = Categorical(nclusters)

get_cluster = rand_function(ClusterType)
get_point = rand_function(ClusterType, PointType)

def sample():
  cluster = get_cluster()
  return (cluster, get_point(cluster))
\end{verbatim}

In this example, we cluster 2d points into 3 different clusters.  Given a cluster, the distribution for a point is some independent Gaussian distribution.  This is similar to fuzzy c-means clustering.


  \subsection{Factor Analysis}

\begin{verbatim}
num_components = 3
point_dim = 5

ComponentsType = Vector(num_components, Double)
PointType = Vector(point_dim, Double)

get_point = randFunction(ComponentsType, PointType)

def sample():
  components = [normal(0, 1) for i in range(num_components)]
  return (comenents, get_point(components))
\end{verbatim}

The factor analysis model is very similar to the clustering model.  The main difference is that we replace the categorical \texttt{ClusterType} type with a vector type.  This results in the model attempting to find each point as an affine function of a vector of standard normal values.

  \subsection{Hidden Markov model}
\begin{verbatim}
num_states = 5
num_observations = 3
sequence_length = 10

StateType = Categorical(num_states)
ObsType = Categorical(num_observations)

trans_fun = rand_function(StateType, StateType)
obs_fun = rand_function(StateType, ObsType)

def sample():
  states = [uniform(num_states)]
  for i in range(sequence_length - 1):
    states.append(trans_fun(states[-1]))
  return (states, [obs_fun(s) for s in states])
\end{verbatim}

In this example, we use the unknown function \texttt{trans\_fun} for state transitions and \texttt{obs\_fun} for observations.  This means that we will learn both the state transitions and the observation distribution.

\subsection{Latent Dirichlet Allocation}

\begin{verbatim}
n_classes = 10
n_words = 10000
n_words_per_document = 1000

ClassType = Categorical(n_classes)
WordType = Categorical(n_words)

class_to_word = rand_function(ClassType, WordType)

def sample():
    which_class = uniform_categorical(n_classes)
    words = [class_to_word(which_class) for i in range(n_words_per_document)]
    return (which_class, words)
\end{verbatim}

  \section{Discussion}

  - Comments about results
  - Future directions
    - Unbounded models
    - Recursive algebraic data types


\end{document}

